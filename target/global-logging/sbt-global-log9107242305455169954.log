[0m[[0m[0mdebug[0m] [0m[0m> Exec(collectAnalyses, None, Some(CommandSource(network-1)))[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Processing event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: initialized: JsonRpcNotificationMessage(2.0, initialized, {})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///home/dblab/ysh/clustering/src/main/scala/com/example/clustering/App.scala","languageId":"scala","version":1,"text":"package com.example.clustering\n\n/**\n * @author ${user.name}\n */\n\nimport org.apache.spark.sql.SparkSession\nimport scala.io.Source\nimport org.apache.spark.rdd.RDD\nimport scala.util.control._\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.rdd.RDD // RDD 형식 파일\nimport org.apache.spark._\nimport org.apache.spark.sql.{Row,SparkSession,SQLContext,DataFrame}\nimport org.apache.spark.sql.functions.lit\nimport org.apache.log4j.{Level, LogManager, Logger}\nimport org.apache.spark.sql.types.{DataTypes,StructType, StructField, StringType, IntegerType, FloatType, MapType, DoubleType, ArrayType}\nimport org.apache.spark.sql.functions.{col,from_json,split,explode,abs,exp,pow,sqrt,broadcast}\nimport scala.collection.mutable.Map\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.sql.functions._\nimport java.util.concurrent.TimeUnit.NANOSECONDS\nimport scala.collection.immutable._\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions.row_number\nimport java.util.ArrayList\nimport scala.collection.mutable\nimport scala.util.control.Breaks._\nimport java.math.MathContext\nimport org.apache.spark.ml.clustering.KMeans\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\nimport org.apache.spark.sql.functions.lit\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.stat.Summarizer\nimport org.apache.spark.ml.feature.{StringIndexer, VectorAssembler}\nimport breeze.linalg.DenseMatrix\nimport org.apache.spark.mllib.recommendation.Rating\n\nobject App {\n  def main(args: Array[String]): Unit = {\n    val spark = SparkSession.builder()\n                                .appName(\"Clustering Application\")\n                                .getOrCreate()\n\n    val sc = spark.sparkContext\n    val sqlContext = new org.apache.spark.sql.SQLContext(sc) //데이터 프레임\n    sqlContext.setConf(\"spark.sql.shuffle.partitions\",\"40\")\n    import sqlContext.implicits._ // 데이터 프레임 라이브러리       \n    \n    val feature_path_query = \"/data/query_rows.csv\"\n    val feature_path_whole = \"/data/millon.csv\"\n\n    val img_list_query = data_process(feature_path_query, sc).flatMap(_._2)\n    val img_list_whole = data_process(feature_path_whole, sc)\n\n    val point = 1000\n    //val list = img_list_query.collect.map(x => ((x * point).toInt))\n    val list = img_list_query.collect.map(x => x)\n    //시간 측정\n    var start = System.nanoTime() //나노 초로 시간 측정 \n        \n    val res = img_list_whole.map { line => \n    //val data = line._2.map(x => ((x * point).toInt))\n    val data = line._2.map(x => x)\n    val data_res = rbf_kernel(data, list)\n    (line._1 , data_res)\n    } \n    println(\"\\n\\n\")\n    var end = System.nanoTime()  // 나노 초로 끝 시간 측\n    res.sortBy(_._2).take(10).foreach(println) \n    println(\"\\n\\n\")\n    println(\"==========================================================================================\")\n    println(\"                                      RBF Kernel                                          \")\n    println(\"==========================================================================================\")     \n    println(\"Completed\")\n    println(\"\\n\\n\")\n    println(\"==========================================================================================\")\n    println(\"                                      Time Taken                                          \")\n    println(\"==========================================================================================\")\n    println(s\"Time Taken: ${NANOSECONDS.toNanos(end-start)}ns\")\n    println(\"\\n\\n\")\n    sc.stop()\n    //SparkSession 종료     \n\n    spark.stop()\n  }\n  def data_process (feature_file : String, sc : SparkContext) : RDD[(String , Array[Float])] = {   \n    val slength = 4096\n\n    val feature_RDD = sc.textFile(feature_file, minPartitions=8) //RDD생성\n\n    val list = feature_RDD.map { x => \n    val spl = x.replace(\"\\\\[|\\\\]\",\"\")\n                .replace(\"(\",\"\")\n                .replace(\"))\",\"\")\n                .replace(\"Vector\",\"\").split(\",\")\n    val key = spl(0)\n    val fvals = spl.slice(1,slength).map(x => x.toFloat)    \n    (key,fvals)\n    }\n    return list\n  }\n  def rbf_kernel(fv1 : Array[Float], fv2 : Array[Float]) = {\n    // 0.1 , 0.5 , 10\n    val gamma = 0.5\n    val sum = fv1.zip(fv2).map {\n            case (p1, p2) => \n                val d = p2 - p1\n                scala.math.pow(d,2)\n            }.sum \n    val res = scala.math.exp(-gamma / sum)\n    res\n  }\n}"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / collectAnalyses[0m
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: Signal, check cycles: false, forcegc: true[0m
[0m[[0m[0mdebug[0m] [0m[0manalysis location (/home/dblab/ysh/clustering/target/scala-2.12/zinc/inc_compile_2.12.zip,true)[0m
[0m[[0m[32msuccess[0m] [0m[0mTotal time: 1 s, completed Jun 4, 2023 10:06:01 PM[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Done event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0m> Exec(shell, None, None)[0m
[0m[[0m[0mdebug[0m] [0m[0mForcing garbage collection...[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: $/setTraceNotification: JsonRpcNotificationMessage(2.0, $/setTraceNotification, {"value":"off"})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled request received: shutdown: JsonRpcRequestMessage(2.0, ♨1, shutdown, null})[0m
